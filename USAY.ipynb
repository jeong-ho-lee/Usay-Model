{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21a95a142b71e93",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108874429856aabe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:31:43.588834Z",
     "start_time": "2024-06-09T08:31:41.792779Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "\n",
    "# huggingface\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98126af063c251e0",
   "metadata": {},
   "source": [
    "## Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380ee3f9-2c1a-478f-a51b-59ead92a6199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/owiequhf/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(\n",
    "    token=\"\",\n",
    "    add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:31:43.950681Z",
     "start_time": "2024-06-09T08:31:43.590251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/owiequhf/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(\n",
    "    token=\"\", # ADD YOUR TOKEN HERE\n",
    "    add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29fda50194a3fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5555b5512cd1a1dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:31:44.607669Z",
     "start_time": "2024-06-09T08:31:43.952076Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "device = (\n",
    "    \"cuda:0\" if torch.cuda.is_available() else # Nvidia GPU\n",
    "    \"mps\" if torch.backends.mps.is_available() else # Apple Silicon GPU\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Device = {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e72bfaf9471e1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:31:44.662961Z",
     "start_time": "2024-06-09T08:31:44.609018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Implementation = flash_attention_2\n"
     ]
    }
   ],
   "source": [
    "# Flash Attention Implementation\n",
    "if device == \"cuda:0\":\n",
    "    if torch.cuda.get_device_capability()[0] >= 8: # Ampere, Ada, or Hopper GPUs\n",
    "        attn_implementation = \"flash_attention_2\"\n",
    "        torch_dtype = torch.bfloat16\n",
    "    else:\n",
    "        attn_implementation = \"eager\"\n",
    "        torch_dtype = torch.float16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float32\n",
    "print(f\"Attention Implementation = {attn_implementation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b452e9ab5ed33883",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "778930f2cc7c1224",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:31:44.667133Z",
     "start_time": "2024-06-09T08:31:44.664135Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Tokenizer parameters\n",
    "################################################################################\n",
    "max_length=8192\n",
    "padding=\"do_not_pad\"  # \"max_length\", \"longest\", \"do_not_pad\"\n",
    "truncation=True\n",
    "\n",
    "################################################################################\n",
    "# Generation parameters\n",
    "################################################################################\n",
    "num_return_sequences=1\n",
    "max_new_tokens=1024\n",
    "do_sample=True  # True for sampling, False for greedy decoding\n",
    "temperature=0.6\n",
    "top_p=0.9\n",
    "repetition_penalty=1.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "load_in_4bit=True\n",
    "bnb_4bit_compute_dtype=torch_dtype\n",
    "bnb_4bit_quant_type=\"nf4\"  # \"nf4\", #fp4\"\n",
    "bnb_4bit_use_double_quant=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f31dc8d02b7233",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352852804a6a38db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:31:44.677584Z",
     "start_time": "2024-06-09T08:31:44.668096Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Model ID\n",
    "model_id = \"beomi/Llama-3-KoEn-8B-Instruct-preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1ea98f6eba58f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:31:45.123540Z",
     "start_time": "2024-06-09T08:31:44.678725Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be26d2efc49fcbf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:31:45.128535Z",
     "start_time": "2024-06-09T08:31:45.124645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f879cefa518232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:32:54.383854Z",
     "start_time": "2024-06-09T08:31:45.129553Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c50bc09757940578e2c12154ce0d045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    attn_implementation=attn_implementation,\n",
    "    torch_dtype=torch_dtype,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fedd479c0a753a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:32:54.389677Z",
     "start_time": "2024-06-09T08:32:54.385445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the model architecture\n",
    "display(Markdown(f'```{model}```'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ae15c79b7731f1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:32:54.400190Z",
     "start_time": "2024-06-09T08:32:54.390662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters (in billions): 8.03\n"
     ]
    }
   ],
   "source": [
    "# Number of parameters\n",
    "print(f\"Number of parameters (in billions): {model.num_parameters() / 1e9:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7e2fc01311961",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5856d3cc7e89850b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:44:14.512665Z",
     "start_time": "2024-06-09T08:44:14.508919Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_question():\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a Korean interviewer. \"\n",
    "                                      \"You are interviewing a candidate for a computer science position. \"\n",
    "                                      \"Please ask a question related to computer science. \"\n",
    "                                      \"Use Korean only. 한국어만 사용하세요.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Please create an open-ended question related to computer science. \"\n",
    "                                    \"Do not provide an answer.\"},\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        eos_token_id=terminators,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"assistant\")[1].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9247112531a9deb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:44:14.697315Z",
     "start_time": "2024-06-09T08:44:14.693828Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are the interviewee. \"\n",
    "                                      \"Please provide an answer to the following question.\"\n",
    "                                      \"Answer should be 3 to 5 sentences long. \"\n",
    "                                      \"Use Korean only. 한국어만 사용하세요.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"###Qustion: {question}\"},\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        eos_token_id=terminators,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"assistant\")[1].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c849786ee0282d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:48:05.871332Z",
     "start_time": "2024-06-09T08:48:05.867843Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(question, context, answer):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an interviewer. \"\n",
    "                                      \"Use Korean only. 한국어만 사용하세요.\"\n",
    "                                      \"First, observe the 'Question'. \"\n",
    "                                      \"Then, based on the 'Context', evaluate the 'User Answer' in a scale of 0 to 5.\"\n",
    "                                      \"Explain the reasoning behind your evaluation and provide feedback.\"\n",
    "                                      \"###Output format: \"\n",
    "                                      \"**평가**: + 평가 내용 + \\n\"\n",
    "                                      \"**이유**: + 이유 내용 + \\n\"\n",
    "                                      \"**피드백**: + 피드백 내용 + \\n\"}, \n",
    "        {\"role\": \"user\", \"content\": f\"###Question: + {question} + \\n\"\n",
    "                                    f\"###Context: + {context} + \\n\"\n",
    "                                    f\"###User Answer: + {answer} + \\n\"},\n",
    "    ]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"assistant\")[1].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79f800de51afcdb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:45:41.665868Z",
     "start_time": "2024-06-09T08:45:38.829201Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the question: \"컴퓨터 과학에서 '이산 공간'과 '연속 공간'의 차이점을 설명하고, 이 개념을 프로그래밍에 적용하는 방법을 예를 들어 설명하십시오.\"\n"
     ]
    }
   ],
   "source": [
    "# 사용자에게 주어지는 첫 번째 질문\n",
    "first_question = generate_question()\n",
    "print(first_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5fc102538fad53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:45:47.748268Z",
     "start_time": "2024-06-09T08:45:41.667211Z"
    }
   },
   "outputs": [],
   "source": [
    "# 사용자에게 주어지는 첫 번째 질문에 대한 완벽한 답변 (모델이 생성, 사용자에게 보여지지 않음)\n",
    "perfect_answer = generate_answer(first_question)\n",
    "print(perfect_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1386d42b3c8ec7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:48:07.728491Z",
     "start_time": "2024-06-09T08:48:07.725811Z"
    }
   },
   "outputs": [],
   "source": [
    "# 사용자의 답변\n",
    "user_answer = \"별로라고 생각합니다\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e9beaf419a3f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T08:48:16.818220Z",
     "start_time": "2024-06-09T08:48:07.975198Z"
    }
   },
   "outputs": [],
   "source": [
    "# 사용자의 답변에 대한 평가 및 피드백\n",
    "first_response = generate_response(first_question, perfect_answer, user_answer)\n",
    "print(first_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4ee3b-3ef5-49e4-86f2-22be143ee31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://163.180.160.32:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, Response\n",
    "from flask_cors import CORS\n",
    "from flask_session import Session\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "# 임시 세션 저장소 (초기 질문 저장용 - 배포시 반드시 분리)\n",
    "session_store = {}\n",
    "\n",
    "\n",
    "# init에서의 응답은 면접자 와 컴퓨터가 동시에 대답해야하는 질문 : 전역으로 저장해둠 -> interview에서 string을 조합할때 question에서 씀\n",
    "@app.route(\"/init\", methods=[\"GET\"])\n",
    "def init():\n",
    "    system_prompt = \"당신은 테크 기업 개발자의 기술면접관 입니다. 컴퓨터공학 전공 지식과 관련된 간단한 면접 질문을 시작하세요.\"\n",
    "\n",
    "    init_question = generate_question()\n",
    "\n",
    "    #초기 질문에 대한 사전 답변 미리 생성\n",
    "    model_answer = generate_answer(init_question)\n",
    "    session_store[1] = {'init_question': init_question , 'model_answer' : model_answer}\n",
    "    return Response(init_question, content_type=\"text/plain\")\n",
    "\n",
    "\n",
    "@app.route(\"/ask\", methods=[\"POST\"])\n",
    "def ask():\n",
    "    # 임시 세션 저장소 - 초기 질문 저장용\n",
    "    session_id = 1\n",
    "    init_question = session_store[session_id].get('init_question')\n",
    "    model_answer = session_store[session_id].get('model_answer')\n",
    "    \n",
    "    params = request.get_json()\n",
    "    # system_prompt = \"'Context' 의 내용을 바탕으로 질문 'Question' 에 대한 답변 'Answer' 가 5점 만점에 몇 점짜리 답변인지 평가하고 그 이유를 설명해주세요. 그리고 만점을 받기 위해서 답변을 어떻게 개선해야할지 설명해주세요.\"\n",
    "    user_answer = params[\"answer\"]  # prompt :사용자가 답변한 내용.\n",
    "    # context: init에서만든질문을 모델이답변 question: init에서 얻은 질문  answer: 사용자 답변\n",
    "\n",
    "    evaluate_result = generate_response(init_question, model_answer, user_answer) # 답변평가용 (세개 인자)\n",
    "    # 질문 = generate_question() - init_question\n",
    "    # 인공지능답변 = generate_answer(question) - model_answer\n",
    "    # 질문_인공지능_사용자_평가결과 =generate_response(question, context, answer)\n",
    "    return Response(evaluate_result, content_type=\"text/plain\")\n",
    "\n",
    "\n",
    "app.run(host=\"163.180.160.32\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fcc71-6fc0-4f56-afd0-924f6686ffd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
